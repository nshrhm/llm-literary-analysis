# Project Intelligence

## Key Implementation Paths

-   `experiment_runner.py` contains the unified experiment runner for both Gemini and Claude models:
    - `BaseExperimentRunner`: Common functionality for all LLM experiments
    - `GeminiExperimentRunner`: Gemini-specific implementation
    - `ClaudeExperimentRunner`: Claude-specific implementation
-   `check_models.py` provides model availability checking for both LLMs
-   `parameters.py` defines personas, texts, and model configurations
-   `aggregate_experiment_results.py` is used for aggregating and analyzing experiment results

## Project-Specific Patterns

### Model Organization
-   Models are organized by LLM type in `parameters.py`:
    ```python
    GEMINI_MODELS = {
        "gemini20pe": "gemini-2.0-pro-exp",
        "gemini15f": "gemini-1.5-flash-8b-latest",
        ...
    }
    CLAUDE_MODELS = {
        "claude37s": "claude-3-7-sonnet-20250219",
        "claude30h": "claude-3-haiku-20240307",
        ...
    }
    ```

### Result Format
-   The experiment results contain numerical evaluations (0-100) and reasons for four emotions:
    - 面白さ (Interesting/Fun)
    - 驚き (Surprise)
    - 悲しみ (Sadness)
    - 怒り (Anger)
-   Results are formatted with clear question numbers and labels:
    ```
    Q1. 面白さ(数値): [0-100]
    Q1. 面白さ(理由): [explanation]
    ```

## File Organization

-   The project uses a unified architecture with a base experiment runner.
-   All prompts and text content are in Japanese.
-   Results are organized by LLM type:
    ```
    results/
    ├── gemini/          # Gemini experiment results
    └── claude/          # Claude experiment results
    ```
-   Results file naming follows the pattern:
    `p{persona}_{model}_n{trial}_temp{temp}_t{text}.txt`
    - persona: p1-p4 (大学１年生、文学研究者、感情豊かな詩人、無感情なロボット)
    - model: Model identifier (e.g., gemini15f, claude30h)
    - trial: Trial number (e.g., n01)
    - temp: Temperature value (e.g., temp50 for 0.5)
    - text: t1-t3 (懐中時計、お金とピストル、ぼろぼろな駝鳥)

## Data Processing

### Result Aggregation
-   Results are aggregated using `aggregate_experiment_results.py`
-   The script processes text files and outputs CSV format with the following columns:
    - Metadata: timestamp, persona, model, trial, temperature, text
    - Evaluations: Q1value-Q4value (numerical scores)
    - Reasons: Q1reason-Q4reason (textual explanations)

### File Processing
-   Text files are processed using regular expressions to extract data
-   Multiple input formats are supported for compatibility
-   Results are validated and errors are logged during processing

## Known Challenges

-   API rate limits and costs may be a constraint when running the experiment.
-   The availability of the LLM models may vary.
-   Managing and comparing results across different model versions requires careful organization.

## Tool Usage Patterns

-   BaseExperimentRunner provides common functionality:
    - `generate_prompt`: Creates standardized prompts
    - `extract_value/reason`: Processes model responses
    - `save_result`: Handles result file creation
-   Model-specific runners handle API interactions:
    - GeminiExperimentRunner uses google.generativeai
    - ClaudeExperimentRunner uses anthropic
-   `check_models.py` provides unified model checking:
    - Lists configured models
    - Verifies model availability
    - Shows model capabilities (Gemini)
